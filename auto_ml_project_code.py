# -*- coding: utf-8 -*-
"""Chethan_B_Auto_ML_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10p2B7uq-hkgoGtH-SC0-NFSEO4jo6C5V
"""

pip install pyngrok

pip install streamlit

# Commented out IPython magic to ensure Python compatibility.
# %%writefile my_application.py
# import json
# import pandas as pd
# import streamlit as st
# from sklearn.impute import SimpleImputer
# from sklearn.decomposition import PCA
# from sklearn.model_selection import train_test_split, GridSearchCV
# from sklearn.metrics import mean_absolute_error, r2_score
# from sklearn.linear_model import LinearRegression
# from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
# from sklearn.svm import SVR, SVC
# from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier
# from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier
# from sklearn.preprocessing import StandardScaler, OneHotEncoder
# from sklearn.compose import ColumnTransformer
# 
# # Model selection based on prediction type
# def choose_model(prediction_type):
#     available_models = []
# 
#     if prediction_type == "Regression":
#         available_models = [
#             ('LinearRegression', LinearRegression()),
#             ('RandomForestRegressor', RandomForestRegressor()),
#             ('SVR', SVR()),
#             ('DecisionTreeRegressor', DecisionTreeRegressor()),
#             ('KNeighborsRegressor', KNeighborsRegressor())
#         ]
#     elif prediction_type == "Classification":
#         available_models = [
#             ('RandomForestClassifier', RandomForestClassifier()),
#             ('SVC', SVC()),
#             ('DecisionTreeClassifier', DecisionTreeClassifier()),
#             ('KNeighborsClassifier', KNeighborsClassifier())
#         ]
#     else:
#         raise ValueError(f"Unknown prediction type: {prediction_type}")
# 
#     return available_models
# 
# # Load configuration from JSON
# def load_json(file_path):
#     with open(file_path, 'r') as file:
#         return json.load(file)
# 
# # Handle missing data by imputing
# def impute_missing_data(X):
#     numeric_features = X.select_dtypes(include=['float64', 'int64']).columns
#     categorical_features = X.select_dtypes(include=['object']).columns
# 
#     num_imputer = SimpleImputer(strategy='mean')
#     cat_imputer = SimpleImputer(strategy='most_frequent')
# 
#     transformer = ColumnTransformer(
#         transformers=[('num', num_imputer, numeric_features),
#                       ('cat', cat_imputer, categorical_features)])
# 
#     X_imputed = transformer.fit_transform(X)
#     X_imputed = pd.DataFrame(X_imputed, columns=numeric_features.tolist() + categorical_features.tolist())
# 
#     return X_imputed
# 
# # Feature reduction based on the configuration
# def reduce_features(X, y, reduction_config):
#     reduction_method = reduction_config.get("reduction_type", "No Reduction")
# 
#     if reduction_method == "No Reduction":
#         return X
#     elif reduction_method == "Corr with Target":
#         correlation = X.corrwith(y)
#         selected_features = correlation[abs(correlation) > 0.2].index
#         return X[selected_features]
#     elif reduction_method == "Tree-based":
#         model = RandomForestRegressor()
#         model.fit(X, y)
#         feature_importance = model.feature_importances_
#         selected_features = X.columns[feature_importance > 0.05]
#         return X[selected_features]
#     elif reduction_method == "PCA":
#         pca = PCA(n_components=0.95)
#         X_reduced = pca.fit_transform(X)
#         return pd.DataFrame(X_reduced)
#     else:
#         return X
# 
# # Perform hyperparameter tuning
# def optimize_hyperparameters(model, param_grid, X_train, y_train):
#     grid_search = GridSearchCV(model, param_grid, cv=5)
#     grid_search.fit(X_train, y_train)
#     return grid_search.best_estimator_
# 
# # Train and evaluate multiple models
# def evaluate_models(models, X_train, X_test, y_train, y_test):
#     evaluation_results = []
#     top_model = None
#     top_r2_score = float('-inf')  # Initializing with lowest possible R2 score
# 
#     for model_name, model in models:
#         param_grid = {'n_estimators': [100, 200], 'max_depth': [10, 20, None]} if isinstance(model, RandomForestRegressor) else {}
# 
#         best_model = optimize_hyperparameters(model, param_grid, X_train, y_train)
#         predictions = best_model.predict(X_test)
# 
#         mae = mean_absolute_error(y_test, predictions)
#         r2 = r2_score(y_test, predictions)
# 
#         evaluation_results.append((model_name, mae, r2))
# 
#         if r2 > top_r2_score:
#             top_r2_score = r2
#             top_model = model_name
# 
#     return evaluation_results, top_model, top_r2_score
# 
# # Streamlit interface
# st.title("Automated Machine Learning App")
# 
# st.sidebar.header("Upload Configuration and Dataset")
# config_file = st.sidebar.file_uploader("Upload JSON Configuration", type="json")
# data_file = st.sidebar.file_uploader("Upload CSV Dataset", type="csv")
# 
# if config_file and data_file:
#     # Load the JSON configuration and the dataset
#     config = json.load(config_file)
#     dataset = pd.read_csv(data_file)
# 
#     target_info = config['design_state_data']['target']
#     target_column = target_info['target']
#     prediction_type = target_info['prediction_type']
#     feature_config = config['design_state_data'].get('features', {})
#     feature_reduction_config = config['design_state_data'].get('feature_reduction', {})
# 
#     st.write(f"**Target Column**: {target_column}")
#     st.write(f"**Prediction Type**: {prediction_type}")
# 
#     # Prepare data
#     X = dataset.drop(columns=[target_column])
#     y = dataset[target_column]
#     X = impute_missing_data(X)
#     X = reduce_features(X, y, feature_reduction_config)
# 
#     # Split the data into training and testing sets
#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# 
#     numeric_columns = X.select_dtypes(include=['float64', 'int64']).columns
#     categorical_columns = X.select_dtypes(include=['object']).columns
# 
#     preprocessor = ColumnTransformer(
#         transformers=[('num', StandardScaler(), numeric_columns),
#                       ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_columns)])
# 
#     X_train = preprocessor.fit_transform(X_train)
#     X_test = preprocessor.transform(X_test)
# 
#     # Select and train models
#     models = choose_model(prediction_type)
#     results, best_model, best_r2 = evaluate_models(models, X_train, X_test, y_train, y_test)
# 
#     # Display model evaluation
#     st.write("### Model Performance")
#     for model_name, mae, r2 in results:
#         st.write(f"**{model_name}**: MAE = {mae:.4f}, R² = {r2:.4f}")
# 
#     st.write(f"### Best Model: **{best_model}** with R² = {best_r2:.4f}")
# 
# else:
#     st.write("Please upload both a JSON configuration file and a CSV dataset.")
#

from pyngrok import ngrok

# Authenticate with your ngrok token if required
ngrok.set_auth_token("2oJvqsQoL0vTQ6UZhwtTzpMkn4X_6aojdU5pHjZt92wwjfYEv")  # Replace with your actual ngrok auth token

# Open the HTTP tunnel to the Streamlit app on port 8501
public_url = ngrok.connect(addr="8501", proto="http")
print(f"Access your Streamlit app at: {public_url}")

!streamlit run my_application.py & npx localtunnel --port 8501